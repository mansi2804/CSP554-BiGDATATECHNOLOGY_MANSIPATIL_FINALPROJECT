# -*- coding: utf-8 -*-
"""BDT_PROJECT1_MP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zWihfqoITkl6c4T-sbHhhSmQ9bcteMVY
"""

!pip install matplotlib-venn

!pip install pyspark

!pip install findspark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("YourAppName") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()

# Import libraries
import findspark
findspark.init()

from pyspark.mllib.recommendation import *
import random
from operator import *
from collections import defaultdict

# Initialize Spark Context
# YOUR CODE GOES HERE
from pyspark import SparkContext, SparkConf
spark = SparkContext.getOrCreate()
spark.stop()
spark = SparkContext('local','Recommender')

# Import test files from location into RDD variables
artistData = spark.textFile('/content/artist_data_small.txt').map(lambda s:(int(s.split("\t")[0]),s.split("\t")[1]))
artistAlias = spark.textFile('/content/artist_alias_small.txt')
userArtistData = spark.textFile('/content/user_artist_data_small.txt')

userArtistData = userArtistData.map(lambda s:(int(s.split(" ")[0]),int(
s.split(" ")[1]),int(s.split(" ")[2])))

# Assuming userArtistData is a tuple of three elements (userid, artistid, playcount)
# If it's different, adjust the code accordingly

# Mapping userArtistData
userArtistData = userArtistData.map(lambda s: (int(s[0]), int(s[1]), int(s[2])))

# Create a dictionary of the 'artistAlias' dataset
artistAliasDictionary = {}
dataValue = artistAlias.map(lambda s: (int(s.split("\t")[0]), int(s.split("\t")[1])))
for temp in dataValue.collect():
    artistAliasDictionary[temp[0]] = temp[1]

# If artistid exists, replace with artistsid from artistAlias, else retain original
userArtistData = userArtistData.map(lambda x: (x[0], artistAliasDictionary.get(x[1], x[1]), x[2]))

# Create an RDD consisting of 'userid' and 'playcount' objects of the original tuple
userSum = userArtistData.map(lambda x: (x[0], x[2]))

# Count instances by key and store in broadcast variable
playCount1 = userSum.reduceByKey(lambda a, b: a + b)
playCount2 = userSum.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)
playSumAndCount = playCount1.leftOuterJoin(playCount2)

# Compute and display users with the highest playcount along with their mean playcount across artists
playSumAndCount = playSumAndCount.map(lambda x: (x[0], x[1][0], int(x[1][0] / x[1][1] if x[1][1] is not None and x[1][1] != 0 else 0)))

# Display top three users
TopThree = playSumAndCount.top(3, key=lambda x: x[1])
for i in TopThree:
    print('User ' + str(i[0]) + ' has a total play count of ' + str(i[1]) + ' and a mean play count of ' + str(i[2]) + '.')

def modelEval(model, dataset):

    # All artists in the 'userArtistData' dataset
    # YOUR CODE GOES HERE
    AllArtists = spark.parallelize(set(userArtistData.map(lambda x:x[1]).collect()))


    # Set of all users in the current (Validation/Testing) dataset
    # YOUR CODE GOES HERE
    AllUsers = spark.parallelize(set(dataset.map(lambda x:x[0]).collect()))


    # Create a dictionary of (key, values) for current (Validation/Testing) dataset
    # YOUR CODE GOES HERE
    ValidationAndTestingDictionary ={}
    for temp in AllUsers.collect():
        tempFilter = dataset.filter(lambda x:x[0] == temp).collect()
        for item in tempFilter:
            if temp in ValidationAndTestingDictionary:
                ValidationAndTestingDictionary[temp].append(item[1])
            else:
                ValidationAndTestingDictionary[temp] = [item[1]]


    # Create a dictionary of (key, values) for training dataset
    # YOUR CODE GOES HERE
    TrainingDictionary = {}
    for temp in AllUsers.collect():
        tempFilter = trainData.filter(lambda x:x[0] == temp).collect()
        for item in tempFilter:
            if temp in TrainingDictionary:
                TrainingDictionary[temp].append(item[1])
            else:
                TrainingDictionary[temp] = [item[1]]


    # For each user, calculate the prediction score i.e. similarity between predicted and actual artists
    # YOUR CODE GOES HERE
    PredictionScore = 0.00
    for temp in AllUsers.collect():
        ArtistPrediction =  AllArtists.map(lambda x:(temp,x))
        ModelPrediction = model.predictAll(ArtistPrediction)
        tempFilter = ModelPrediction.filter(lambda x :not x[1] in TrainingDictionary[x[0]])
        topPredictions = tempFilter.top(len(ValidationAndTestingDictionary[temp]),key=lambda x:x[2])
        l=[]
        for i in topPredictions:
            l.append(i[1])
        PredictionScore+=len(set(l).intersection(ValidationAndTestingDictionary[temp]))/len(ValidationAndTestingDictionary[temp])


    # Print average score of the model for all users for the specified rank
    # YOUR CODE GOES HERE
    print("The model score for rank "+str(model.rank)+" is ~"+str(PredictionScore))

trainData, validationData, testData = userArtistData.randomSplit((0.4,0.4,0.2),seed=13)
trainData.cache()
validationData.cache()
testData.cache()

# Display the first 3 records of each dataset followed by the total count of records for each datasets
print(trainData.take(3))
print(validationData.take(3))
print(testData.take(3))
print(trainData.count())
print(validationData.count())
print(testData.count())

def modelEval(model, dataset):

    # All artists in the 'userArtistData' dataset

    AllArtists = spark.parallelize(set(userArtistData.map(lambda x:x[1]).collect()))


    # Set of all users in the current (Validation/Testing) dataset

    AllUsers = spark.parallelize(set(dataset.map(lambda x:x[0]).collect()))


    # Create a dictionary of (key, values) for current (Validation/Testing) dataset

    ValidationAndTestingDictionary ={}
    for temp in AllUsers.collect():
        tempFilter = dataset.filter(lambda x:x[0] == temp).collect()
        for item in tempFilter:
            if temp in ValidationAndTestingDictionary:
                ValidationAndTestingDictionary[temp].append(item[1])
            else:
                ValidationAndTestingDictionary[temp] = [item[1]]


    # Create a dictionary of (key, values) for training dataset

    TrainingDictionary = {}
    for temp in AllUsers.collect():
        tempFilter = trainData.filter(lambda x:x[0] == temp).collect()
        for item in tempFilter:
            if temp in TrainingDictionary:
                TrainingDictionary[temp].append(item[1])
            else:
                TrainingDictionary[temp] = [item[1]]


    # For each user, calculate the prediction score i.e. similarity between predicted and actual artists

    PredictionScore = 0.00
    for temp in AllUsers.collect():
        ArtistPrediction =  AllArtists.map(lambda x:(temp,x))
        ModelPrediction = model.predictAll(ArtistPrediction)
        tempFilter = ModelPrediction.filter(lambda x :not x[1] in TrainingDictionary[x[0]])
        topPredictions = tempFilter.top(len(ValidationAndTestingDictionary[temp]),key=lambda x:x[2])
        l=[]
        for i in topPredictions:
            l.append(i[1])
        PredictionScore+=len(set(l).intersection(ValidationAndTestingDictionary[temp]))/len(ValidationAndTestingDictionary[temp])

    # Print average score of the model for all users for the specified rank

    print("The model score for rank "+str(model.rank)+" is ~"+str(PredictionScore))

rankList = [2,10,20]
 for rank in rankList:
     model = ALS.trainImplicit(trainData, rank , seed=345)
     modelEval(model,validationData)

bestModel = ALS.trainImplicit(trainData, rank=10, seed=345)
modelEval(bestModel, testData)

# Find the top 5 artists for a particular user and list their names
# YOUR CODE GOES HERE

TopFive = bestModel.recommendProducts(1059637,5)
for item in range(0,5):
    print("Artist "+str(item)+": "+artistData.filter(lambda x:x[0] == TopFive[item][1]).collect()[0][1])

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load user_artist_data.txt
columns = ["userid", "artistid", "playcount"]
user_artist_data = pd.read_csv("user_artist_data_small.txt", sep=" ", header=None, names=columns)

# Load artist_data.txt
artist_columns = ["artistid", "artist_name"]
artist_data = pd.read_csv("artist_data_small.txt", sep="\t", header=None, names=artist_columns)

# Merge user_artist_data with artist_data
merged_data = pd.merge(user_artist_data, artist_data, on="artistid")

# Load artist_alias.txt
alias_columns = ["badid", "goodid"]
artist_alias = pd.read_csv("artist_alias_small.txt", sep="\t", header=None, names=alias_columns)

# Merge alias information into merged_data
merged_data = pd.merge(merged_data, artist_alias, left_on="artistid", right_on="badid", how="left")

# Replace NaN values in goodid column with artistid
merged_data["goodid"].fillna(merged_data["artistid"], inplace=True)

# Group by artist_name and calculate total playcount
artist_playcount = merged_data.groupby("artist_name")["playcount"].sum().reset_index()

# Create a bar plot for the top N artists
top_n = 10
top_artists = artist_playcount.nlargest(top_n, "playcount")

# Plotting
plt.figure(figsize=(12, 6))
sns.barplot(x="playcount", y="artist_name", data=top_artists, palette="viridis")
plt.title(f"Top {top_n} Most Played Artists")
plt.xlabel("Total Playcount")
plt.ylabel("Artist Name")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you've already loaded and processed the data (similar to the previous example)

# Group by artist_name and calculate total playcount
artist_playcount = merged_data.groupby("artist_name")["playcount"].sum().reset_index()

# Create a pie chart for the last 5 artists
bottom_n = 5
bottom_artists = artist_playcount.nsmallest(bottom_n, "playcount")

# Plotting
plt.figure(figsize=(8, 8))
plt.pie(bottom_artists["playcount"], labels=bottom_artists["artist_name"], autopct="%1.1f%%", startangle=90, colors=plt.cm.Paired.colors)
plt.title(f"Distribution of Playcounts Among Bottom {bottom_n} Artists")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


# Merge user_artist_data with artist_data
merged_data = pd.merge(user_artist_data, artist_data, on="artistid")

# Calculate total playcount per artist
artist_playcount = merged_data.groupby("artist_name")["playcount"].sum().reset_index()

# Select top N artists for visualization
top_n = 20
top_artists = artist_playcount.nlargest(top_n, "playcount")

# Generate random colors for each bar
colors = np.random.rand(top_n, 3)

# Plotting
plt.figure(figsize=(10, 6))
plt.barh(top_artists["artist_name"], top_artists["playcount"], color=colors)
plt.xlabel("Playcount")
plt.ylabel("Artist Name")
plt.title(f"Top {top_n} Artists by Playcount")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load user_artist_data.txt
columns = ["userid", "artistid", "playcount"]
user_artist_data = pd.read_csv("user_artist_data_small.txt", sep=" ", header=None, names=columns)

# Load artist_data.txt
artist_columns = ["artistid", "artist_name"]
artist_data = pd.read_csv("artist_data_small.txt", sep="\t", header=None, names=artist_columns)

# Merge user_artist_data with artist_data
merged_data = pd.merge(user_artist_data, artist_data, on="artistid")

# Choose a specific user (replace 'your_user_id' with the desired user ID)
user_id = 1046559

# Filter data for the chosen user
user_data = merged_data[merged_data["userid"] == user_id]

# Calculate playcount for each artist
artist_playcount = user_data.groupby("artist_name")["playcount"].sum().reset_index()

# Select top 5 artists
top_5_artists = artist_playcount.nlargest(5, "playcount")

# Visualize the results
plt.figure(figsize=(10, 6))
plt.barh(top_5_artists["artist_name"], top_5_artists["playcount"], color='skyblue')
plt.xlabel("Playcount")
plt.ylabel("Artist Name")
plt.title(f"Top 5 Artists for User {user_id}")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Merge user_artist_data with artist_data
merged_data = pd.merge(user_artist_data, artist_data, on="artistid")

# Choose a specific user (replace 'your_user_id' with the desired user ID)
user_id = 1052461

# Filter data for the chosen user
user_data = merged_data[merged_data["userid"] == user_id]

# Calculate playcount for each artist
artist_playcount = user_data.groupby("artist_name")["playcount"].sum().reset_index()

# Select top 5 artists
top_5_artists = artist_playcount.nlargest(5, "playcount")

# Plot a pie chart
plt.figure(figsize=(8, 8))
plt.pie(top_5_artists["playcount"], labels=top_5_artists["artist_name"], autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)
plt.title(f"Playcount Distribution for Top 5 Artists (User {user_id})")
plt.show()